{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Normal equation](#Normal-equation)\n",
    "\t* [1) Intuition](#1%29-Intuition)\n",
    "\t* [2) Example](#2%29-Example)\n",
    "\t* [3) Feature Scaling](#3%29-Feature-Scaling)\n",
    "\t* [4) Advantages vs Disadvantages](#4%29-Advantages-vs-Disadvantages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Normal equation</u>**, which for some linear regression problems, will give us a much better way to solve for the optimal value of the parameters theta. \n",
    "\n",
    "So far, the algorithm that we've been using for linear regression is **<u>gradient descent</u>** where in order to minimize the cost function $\\large J_{\\theta}$, we would take this iterative algorithm that takes many steps, multiple iterations of gradient descent to converge to the global minimum.\n",
    "\n",
    "In contrast, the **<u>normal equation</u>** would give us a method to solve for theta analytically, so that rather than needing to run this iterative algorithm, we can instead just solve for the optimal value for theta all at one go, so in basically one step you get to the optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic18.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/2DKxQ/normal-equation) 0:55*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theta is a real number**\n",
    "\n",
    "- Let's take a very simplified cost function $\\large J_{\\theta}$, that is just the function of a real number Theta. You can think of Theta as a scalar value or a real value. Theta is just a number, rather than a vector.\n",
    "\n",
    "- Imagine that we have a cost function J that is a quadratic function of the real value parameter Theta. How do you minimize a quadratic function?\n",
    "- If you know calculus, the way to minimize a function is to take derivatives and to set derivatives equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theta is no longer just a real number, but, instead, is this n+1-dimensional parameter vector**\n",
    "\n",
    "- The cost function J in this case is a function of the vector value from $\\large \\theta_{0} \\rightarrow \\theta_{m}$. So in order to minimize this cost function, we need to take the partial derivative of J, with respect to every parameter of Theta J in turn, and then, to set all of these to 0. If you do that, and you solve for the values of Theta 0, Theta 1, up to Theta N, then, this would give you that values of Theta to minimize the cost function J."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic19.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/2DKxQ/normal-equation) 2:43*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2) Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example, I have m = 4 training examples. I will take the dataset and add an extra column that corresponds to an extra feature, x0, that is always takes on this value of 1, so x0 = 1. Then I will contruct a matrix called X that contains all of the features from my training data.\n",
    "\n",
    "- **X**: m by (n + 1) - dimensional matrix\n",
    "- **y**: m-dimensional vector \n",
    "- **m**: training examples\n",
    "- **n**: number of features\n",
    "\n",
    "X has n + 1 features because of the extra feature x0 that I had.\n",
    "\n",
    "Based on matrix X and vector Y, we can compute the value of theta that minimizes the cost function:\n",
    "$$\\large \\theta = (X^{T}X)^{-1}X^{T}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic20.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/2DKxQ/normal-equation) 5:58*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In a general case, let us say we have M training examples so **($x^{(1)}, y^{(1)}$) up to $(x^{(n)}, y^{(n)})$** and n features. So, each of the training example **$x^{(i)}$** may looks like a vector, that is a **n+1 dimensional feature vector**.\n",
    "\n",
    "Then, I am going to construct a **matrix X**, which is also called a **design matrix**. Steps to construct the design matrix X:\n",
    "- Take the first training example (that is a vector), take its transpose, and make  **$((x^{(1)})^{T}$** the first row of the design matrix.\n",
    "- Take the second training example (that is a vector), take its transpose, and make  **$((x^{(2)})^{T}$** the second row of the design matrix. Do the samething until the last traning example, **$((x^{(m)})^{T}$**, and that is the last row of the design matrix X.\n",
    "- The design matrix X will now become a **m x (n + 1)** matrix\n",
    "\n",
    "The vector y is obtained by taking all of the labels, for example: prices of the houses, and stacking them up in to an **m-dimensional vector**.\n",
    "\n",
    "Finally, having constructed the matrix X and the vector y, we then just compute:\n",
    "$$\\large \\theta = (X^{T}X)^{-1}X^{T}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic23.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/2DKxQ/normal-equation) 8:44*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic21.png\">\n",
    "<img src=\"images/lec4_pic22.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/2DKxQ/normal-equation) 8:44*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3) Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is no need to use feature scaling if we use the normal equation method.**\n",
    "\n",
    "- However, if you choose to use **Gradient Descent**, then feature scaling is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic24.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/2DKxQ/normal-equation) 11:21*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Advantages vs Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**: works well with large number of features.\n",
    "\n",
    "**Normal Equation**: need to compute $(X^{T}X)^{-1}$. $X^{T}X$ is an **n x n** matrix. The computational cost of inverting a matrix rose roughly as the cube of the dimension of the matrix, $O(n^{3})$. So if n the number of features is very large, then computing this quantity can be slow.  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic25.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/2DKxQ/normal-equation) 14:2*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we get to the more complex learning algorithm, for example, when we talk about classification algorithm, like a logistic regression algorithm... The normal  equation method actually do not work for those more sophisticated learning algorithms, so we need to resort to gradient descent for those algorithms.\n",
    "\n",
    "Gradient Descent is a useful algorithm because it works for both linear regression which has a large number of features and for some of the other algorithms. While for those algorithms, the normal equation method just doesn't apply and doesn't work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
