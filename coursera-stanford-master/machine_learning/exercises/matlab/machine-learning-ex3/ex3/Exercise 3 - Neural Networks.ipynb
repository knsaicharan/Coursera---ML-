{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1) ex3.m](#1%29-ex3.m)\n",
    "* [2) Solution of ex3.m](#2%29-Solution-of-ex3.m)    \n",
    "    * [1) lrCostFunction.m](#1%29-lrCostFunction.m)\n",
    "\t* [2) oneVsAll.m](#2%29-oneVsAll.m)\n",
    "    * [3) predictOneVsAll.m](#3%29-predictOneVsAll.m)    \n",
    "* [3) ex3_nn.m](#3%29-ex3_nn.m)   \n",
    "* [4) Solution of ex3_nn.m](#4%29-Solution-of-ex3_nn.m)\n",
    "    * [1) predict.m](#1%29-predict.m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) ex3.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load ex3.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 3 | Part 1: One-vs-all\n",
    "\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear exercise. You will need to complete the following functions \n",
    "%  in this exericse:\n",
    "%\n",
    "%     lrCostFunction.m (logistic regression cost function)\n",
    "%     oneVsAll.m\n",
    "%     predictOneVsAll.m\n",
    "%     predict.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "clear ; close all; clc\n",
    "\n",
    "%% Setup the parameters you will use for this part of the exercise\n",
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "num_labels = 10;          % 10 labels, from 1 to 10   \n",
    "                          % (note that we have mapped \"0\" to label 10)\n",
    "\n",
    "%% =========== Part 1: Loading and Visualizing Data =============\n",
    "%  We start the exercise by first loading and visualizing the dataset. \n",
    "%  You will be working with a dataset that contains handwritten digits.\n",
    "%\n",
    "\n",
    "% Load Training Data\n",
    "fprintf('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "load('ex3data1.mat'); % training data stored in arrays X, y\n",
    "m = size(X, 1);\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "rand_indices = randperm(m);\n",
    "sel = X(rand_indices(1:100), :);\n",
    "\n",
    "displayData(sel);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%% ============ Part 2: Vectorize Logistic Regression ============\n",
    "%  In this part of the exercise, you will reuse your logistic regression\n",
    "%  code from the last exercise. You task here is to make sure that your\n",
    "%  regularized logistic regression implementation is vectorized. After\n",
    "%  that, you will implement one-vs-all classification for the handwritten\n",
    "%  digit dataset.\n",
    "%\n",
    "\n",
    "fprintf('\\nTraining One-vs-All Logistic Regression...\\n')\n",
    "\n",
    "lambda = 0.1;\n",
    "[all_theta] = oneVsAll(X, y, num_labels, lambda);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "\n",
    "%% ================ Part 3: Predict for One-Vs-All ================\n",
    "%  After ...\n",
    "pred = predictOneVsAll(all_theta, X);\n",
    "\n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Solution of ex3.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) lrCostFunction.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load lrCostFunction.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function [J, grad] = lrCostFunction(theta, X, y, lambda)\n",
    "%LRCOSTFUNCTION Compute cost and gradient for logistic regression with \n",
    "%regularization\n",
    "%   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using\n",
    "%   theta as the parameter for regularized logistic regression and the\n",
    "%   gradient of the cost w.r.t. to the parameters. \n",
    "\n",
    "% Initialize some useful values\n",
    "m = length(y); % number of training examples\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "J = 0;\n",
    "grad = zeros(size(theta));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Compute the cost of a particular choice of theta.\n",
    "%               You should set J to the cost.\n",
    "%               Compute the partial derivatives and set grad to the partial\n",
    "%               derivatives of the cost w.r.t. each parameter in theta\n",
    "%\n",
    "% Hint: The computation of the cost function and gradients can be\n",
    "%       efficiently vectorized. For example, consider the computation\n",
    "%\n",
    "%           sigmoid(X * theta)\n",
    "%\n",
    "%       Each row of the resulting matrix will contain the value of the\n",
    "%       prediction for that example. You can make use of this to vectorize\n",
    "%       the cost function and gradient computations. \n",
    "%\n",
    "% Hint: When computing the gradient of the regularized cost function, \n",
    "%       there're many possible vectorized solutions, but one solution\n",
    "%       looks like:\n",
    "%           grad = (unregularized gradient for logistic regression)\n",
    "%           temp = theta; \n",
    "%           temp(1) = 0;   % because we don't add anything for j = 0  \n",
    "%           grad = grad + YOUR_CODE_HERE (using the temp variable)\n",
    "%\n",
    "\n",
    "htheta = sigmoid(X * theta);\n",
    "J = 1/m * sum((-y .* log(htheta)) - ((1 - y) .* log(1 - htheta))) + lambda/(2*m) * sum(theta(2:end) .^2);\n",
    "temp = theta;\n",
    "temp(1) = 0;\n",
    "% bring 1/m outside\n",
    "grad = 1/m * (X' * (htheta - y) + lambda * temp);\n",
    "\n",
    "\n",
    "% =============================================================\n",
    "\n",
    "grad = grad(:);\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) oneVsAll.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load oneVsAll.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function [all_theta] = oneVsAll(X, y, num_labels, lambda)\n",
    "%ONEVSALL trains multiple logistic regression classifiers and returns all\n",
    "%the classifiers in a matrix all_theta, where the i-th row of all_theta \n",
    "%corresponds to the classifier for label i\n",
    "%   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels\n",
    "%   logisitc regression classifiers and returns each of these classifiers\n",
    "%   in a matrix all_theta, where the i-th row of all_theta corresponds \n",
    "%   to the classifier for label i\n",
    "\n",
    "% Some useful variables\n",
    "m = size(X, 1);\n",
    "n = size(X, 2);\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "all_theta = zeros(num_labels, n + 1);\n",
    "\n",
    "% Add ones to the X data matrix\n",
    "X = [ones(m, 1) X];\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: You should complete the following code to train num_labels\n",
    "%               logistic regression classifiers with regularization\n",
    "%               parameter lambda. \n",
    "%\n",
    "% Hint: theta(:) will return a column vector.\n",
    "%\n",
    "% Hint: You can use y == c to obtain a vector of 1's and 0's that tell use \n",
    "%       whether the ground truth is true/false for this class.\n",
    "%\n",
    "% Note: For this assignment, we recommend using fmincg to optimize the cost\n",
    "%       function. It is okay to use a for-loop (for c = 1:num_labels) to\n",
    "%       loop over the different classes.\n",
    "%\n",
    "%       fmincg works similarly to fminunc, but is more efficient when we\n",
    "%       are dealing with large number of parameters.\n",
    "%\n",
    "% Example Code for fmincg:\n",
    "%\n",
    "%     % Set Initial theta\n",
    "%     initial_theta = zeros(n + 1, 1);\n",
    "%     \n",
    "%     % Set options for fminunc\n",
    "%     options = optimset('GradObj', 'on', 'MaxIter', 50);\n",
    "% \n",
    "%     % Run fmincg to obtain the optimal theta\n",
    "%     % This function will return theta and the cost \n",
    "%     [theta] = ...\n",
    "%         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...\n",
    "%                 initial_theta, options);\n",
    "%\n",
    "\n",
    "options = optimset('GradObj', 'on', 'MaxIter', 50);\n",
    "for i=1:num_labels\n",
    "\tinitial_theta = zeros(size(X, 2), 1);\n",
    "\t[theta] = fmincg(@(t)(lrCostFunction(t, X,y==i, lambda)), initial_theta, options);\n",
    "\tall_theta(i,:) = theta';\n",
    "end\n",
    "\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) predictOneVsAll.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load predictOneVsAll.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function p = predictOneVsAll(all_theta, X)\n",
    "%PREDICT Predict the label for a trained one-vs-all classifier. The labels \n",
    "%are in the range 1..K, where K = size(all_theta, 1). \n",
    "%  p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions\n",
    "%  for each example in the matrix X. Note that X contains the examples in\n",
    "%  rows. all_theta is a matrix where the i-th row is a trained logistic\n",
    "%  regression theta vector for the i-th class. You should set p to a vector\n",
    "%  of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2\n",
    "%  for 4 examples) \n",
    "\n",
    "m = size(X, 1);\n",
    "num_labels = size(all_theta, 1);\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "p = zeros(size(X, 1), 1);\n",
    "\n",
    "% Add ones to the X data matrix\n",
    "X = [ones(m, 1) X];\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Complete the following code to make predictions using\n",
    "%               your learned logistic regression parameters (one-vs-all).\n",
    "%               You should set p to a vector of predictions (from 1 to\n",
    "%               num_labels).\n",
    "%\n",
    "% Hint: This code can be done all vectorized using the max function.\n",
    "%       In particular, the max function can also return the index of the \n",
    "%       max element, for more information see 'help max'. If your examples \n",
    "%       are in rows, then, you can use max(A, [], 2) to obtain the max \n",
    "%       for each row.\n",
    "%       \n",
    "\n",
    "htheta = X * all_theta';\n",
    "[temp, p] = max(htheta, [], 2);\n",
    "\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) ex3_nn.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load ex3_nn.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 3 | Part 2: Neural Networks\n",
    "\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear exercise. You will need to complete the following functions \n",
    "%  in this exericse:\n",
    "%\n",
    "%     lrCostFunction.m (logistic regression cost function)\n",
    "%     oneVsAll.m\n",
    "%     predictOneVsAll.m\n",
    "%     predict.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "clear ; close all; clc\n",
    "\n",
    "%% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   % 25 hidden units\n",
    "num_labels = 10;          % 10 labels, from 1 to 10   \n",
    "                          % (note that we have mapped \"0\" to label 10)\n",
    "\n",
    "%% =========== Part 1: Loading and Visualizing Data =============\n",
    "%  We start the exercise by first loading and visualizing the dataset. \n",
    "%  You will be working with a dataset that contains handwritten digits.\n",
    "%\n",
    "\n",
    "% Load Training Data\n",
    "fprintf('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "load('ex3data1.mat');\n",
    "m = size(X, 1);\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "sel = randperm(size(X, 1));\n",
    "sel = sel(1:100);\n",
    "\n",
    "displayData(X(sel, :));\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%% ================ Part 2: Loading Pameters ================\n",
    "% In this part of the exercise, we load some pre-initialized \n",
    "% neural network parameters.\n",
    "\n",
    "fprintf('\\nLoading Saved Neural Network Parameters ...\\n')\n",
    "\n",
    "% Load the weights into variables Theta1 and Theta2\n",
    "load('ex3weights.mat');\n",
    "\n",
    "%% ================= Part 3: Implement Predict =================\n",
    "%  After training the neural network, we would like to use it to predict\n",
    "%  the labels. You will now implement the \"predict\" function to use the\n",
    "%  neural network to predict the labels of the training set. This lets\n",
    "%  you compute the training set accuracy.\n",
    "\n",
    "pred = predict(Theta1, Theta2, X);\n",
    "\n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%  To give you an idea of the network's output, you can also run\n",
    "%  through the examples one at the a time to see what it is predicting.\n",
    "\n",
    "%  Randomly permute examples\n",
    "rp = randperm(m);\n",
    "\n",
    "for i = 1:m\n",
    "    % Display \n",
    "    fprintf('\\nDisplaying Example Image\\n');\n",
    "    displayData(X(rp(i), :));\n",
    "\n",
    "    pred = predict(Theta1, Theta2, X(rp(i),:));\n",
    "    fprintf('\\nNeural Network Prediction: %d (digit %d)\\n', pred, mod(pred, 10));\n",
    "    \n",
    "    % Pause\n",
    "    fprintf('Program paused. Press enter to continue.\\n');\n",
    "    pause;\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Solution of ex3_nn.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) predict.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load predict.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function p = predict(Theta1, Theta2, X)\n",
    "%PREDICT Predict the label of an input given a trained neural network\n",
    "%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the\n",
    "%   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "% Useful values\n",
    "m = size(X, 1);\n",
    "num_labels = size(Theta2, 1);\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "p = zeros(size(X, 1), 1);\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Complete the following code to make predictions using\n",
    "%               your learned neural network. You should set p to a \n",
    "%               vector containing labels between 1 to num_labels.\n",
    "%\n",
    "% Hint: The max function might come in useful. In particular, the max\n",
    "%       function can also return the index of the max element, for more\n",
    "%       information see 'help max'. If your examples are in rows, then, you\n",
    "%       can use max(A, [], 2) to obtain the max for each row.\n",
    "%\n",
    "\n",
    "\n",
    "X = [ones(m, 1) X];\n",
    "a2 = sigmoid(X * Theta1');\n",
    "a2 = [ones(m, 1) a2];\n",
    "htheta = sigmoid(a2 * Theta2');\n",
    "\n",
    "[temp, p] = max(htheta, [], 2);\n",
    "\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave_kernel"
  },
  "language_info": {
   "codemirror_mode": "Octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
