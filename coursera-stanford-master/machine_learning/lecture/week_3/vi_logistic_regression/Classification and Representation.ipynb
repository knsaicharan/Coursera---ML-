{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1) Classification](#1%29-Classification)\n",
    "* [2) Hypothesis Representation](#2%29-Hypothesis-Representation)\n",
    "\t* [1) Hypothesis representation of logistic regression](#1%29-Hypothesis-representation-of-logistic-regression)\n",
    "\t* [2) Interpretation of Hypothesis output](#2%29-Interpretation-of-Hypothesis-output)\n",
    "* [3) Decision Boundary](#3%29-Decision-Boundary)\n",
    "\t* [1) Logistic Regression](#1%29-Logistic-Regression)\n",
    "\t* [2) Decision Boundary](#2%29-Decision-Boundary)\n",
    "    * [3) Non-linear decision boundaries](#3%29-Non-linear-decision-boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification problems, where the variable y that you want to predict is a discrete value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic01.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/wlPeP/classification) 2:21*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we develop a classification algorithm?** \n",
    "\n",
    "** - First example: **\n",
    "- Here's an example of a training set for a classification task for classifying a tumor as malignant or benign with only 2 values 0 (no) or 1 (yes). We can try to apply linear regression to this data set and just try to fit the straight line to the data. We have the hypothesis:\n",
    "$$\\large h_{\\theta}x = \\theta^{T}x$$\n",
    "- If you want to make predictions, you can threshold the classifier outputs at 0.5 (a vertical axis value 0.5). If the hypothesis outputs a value that is greater than or equal to 0.5 you can take y = 1, if it's less than 0.5 you can take y=0.\n",
    "- In this particular example, it looks like linear regression is actually doing something reasonable.\n",
    "\n",
    "** - Second example: **\n",
    "- Let me extend out the horizontal access a little bit and let's say we got one more training example way out there on the right.\n",
    "- Once we've added that extra example over here, if you now run linear regression, you instead get a different straight line fit to the data. If you know threshold hypothesis at 0.5, you end up with a different point, so that everything to the right of this\n",
    "point you predict as positive and everything to the left of that point you predict as negative.\n",
    "- This seems a pretty bad thing for linear regression. It's pretty clear that by adding one example way out here to the right, this example really isn't giving us any new information.\n",
    "\n",
    "**Applying linear regression to a classification problem often isn't a great idea.**\n",
    "- In the first example, before I added this extra training example, previously linear regression was just getting lucky and it got us a hypothesis that worked well for that particular example, but often it isn't a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic02.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/wlPeP/classification) 5:40*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's one other funny thing about what would happen if we were to use linear regression for a classification problem. For classification we know that y is either 0 or 1.But if you are using linear regression where the hypothesis can output values that are much larger than one or less than zero, even if all of your training examples have labels y equals zero or one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic03.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/wlPeP/classification) 6:45*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic04.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/wlPeP/classification) 7:10*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Hypothesis Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Hypothesis representation of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the function we're going to use to represent our hypothesis when we have a classification problem? We would like our classifier to output values that are between 0 and 1, so we should come up with a hypothesis that satisfies this property, that is, predictions are maybe between 0 and 1.\n",
    "\n",
    "- When we are using linear regression, this was the form of a hypothesis: $\\large h_{\\theta}(x) = \\theta^{T}x$.\n",
    "- For logistic regression, I'm going to modify this hypothesis: $\\large g(\\theta^{T}x)$. Where I'm going to define\n",
    "the function g as follows:\n",
    "$$\\large g(z) = \\dfrac{1}{1 + e^{-z}}$$ where z is a real number\n",
    "- This is called the **sigmoid function**, or the **logistic function**, and the term logistic function, that's what gives rise to the name **logistic regression**.\n",
    "- The terms sigmoid function and logistic function are basically synonyms and mean the same thing. So the two terms are basically interchangeable, and either term can be used to refer to this function g.\n",
    "- If we take these two equations and put them together, then here's just an alternative way of writing out the form of my hypothesis.\n",
    "$$\\large h_{\\theta}(x) = \\dfrac{1}{1 + e^{-\\theta^{T}x}}$$\n",
    "- We're gonna plot the sigmoid function g(z) on this figure below. It starts off near 0 andthen it rises until it crosses 0.5 and the origin, and then it flattens out again like so. And you notice that the sigmoid function, while it asymptotes at one and asymptotes at zero, as a z axis, the horizontal axis is z. As z goes to minus infinity, g(z) approaches zero. And as g(z) approaches infinity, g(z) approaches one. And so because g(z) upwards values are between zero and one, we also have that h(x)\n",
    "must be between zero and one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic05.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/RJXfB/hypothesis-representation) 2:30*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Interpretation of Hypothesis output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how I'm going to interpret the output of my hypothesis, h(x). When my hypothesis outputs some number, I am going to treat that number as the estimated probability that y is equal to one on a new input, example x. \n",
    "\n",
    "- Let's say we're using the tumor classification example, so we may have a feature vector x, which is this x zero equals one as always. And then one feature is the size of the tumor.\n",
    "\n",
    "$$ x = \n",
    "\\begin{bmatrix}\n",
    "    x_{0}\\\\\n",
    "    x_{1}\\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    \\text{tumorSize}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Suppose I have a patient come in and they have some tumor size and I feed their feature vectorx into my hypothesis. And suppose my hypothesis outputs the number 0.7. I'm gonna say that this hypothesis is telling me that for a patient with features x, the probability that y equals 1 is 0.7. In other words, I'm going to tell my patient that the tumor, sadly, has a 70 percent chance, ora 0.7 chance of being malignant.\n",
    "\n",
    "- To write this out slightly more formally in math: $\\large h_{\\theta}(x) = p(y=1 | x;\\theta)$. Here's how I read this expression: This is the probability that y is equal to one. Given x, given that my patient has features x, or given my patient has a particular tumor size represented by my features x. And this probability is parameterized by theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic07.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/RJXfB/hypothesis-representation) 6:00*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic06.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/RJXfB/hypothesis-representation) 5:38*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have hypothesis: $\\large h_{\\theta}(x) = g(\\theta^{T}x)$, where g is the sigmoid function: $\\large g(z) = \\dfrac{1}{1 + e^{-z}}$. In the plot, the sigmoid function slowly increases from zero to one, asymptoting at one.\n",
    "\n",
    "- What I want to do now is try to understand better when this hypothesis will make predictions that y is equal to 1 versus when it might make predictions that y is equal to 0. We know that hypothesis $h_{\\theta}(x) = p(y=1 | x;\\theta)$. Concretely, this hypothesis is outputting estimates of the probability that y is equal to one, given x and parameterized by theta.\n",
    "\n",
    "- So if we wanted to predict is y equal to one or is y equal to zero, here's something we might do. Whenever the hypothesis outputs that the probability of y being one is greater than or equal to 0.5, so this means that if there is more likely to be y equals 1 than y equals 0, then let's predict y equals 1. And otherwise, if the probability, the estimated probability of y being over 1 is less than 0.5, then let's predict y equals 0.\n",
    "\n",
    "- If we take a look at the plot of the sigmoid function, we notice that:\n",
    "    - sigmoid function is greater than or equal to 0.5 when z is positive ($g(z) \\geq 0.5 \\text{ when } z \\geq 0$). Since the hypothesis for logistic regression $h_{\\theta}(x) = g(\\theta^{T}x) \\geq 0.5 \\text{ whenever } \\theta^{T}x \\geq 0$.\n",
    "    - Let's now consider the other case of when a hypothesis will predict y is equal to 0. By similar argument, h(x) is going to be less than 0.5 whenever g(z) is less than 0.5 because the range of values of z that cause g(z) to take on values less than 0.5, well, that's when z is negative. So when g(z) is less than 0.5, a hypothesis will predict that y is equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic08.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/WuL1H/decision-boundary) 4:30*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's suppose we have a training set like that shown on the slide. And suppose a hypothesis is  \n",
    "$$\\large h_{\\theta}(x) = g(\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2})$$\n",
    "- We choose $\\theta_{0}= - 3 $ , $\\theta_{1}= 1 $ ,  $\\theta_{2}= 1 $. This means that my parameter vector is going to be $ \\theta = \n",
    "\\begin{bmatrix}\n",
    "    -3\\\\\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "- Using the formula on the previous slide, we know that y equals one is more likely, that is the probability that y equals one is greater than or equal to 0.5, whenever theta transpose x is greater than zero.\n",
    "$$ \\text{Predict \"y = 1\" if } -3 + x_{1} + x_{2} \\geq 0  \\text{ or } \\theta^{T}x \\geq 0 $$\n",
    "- We can rewrite this as $ x_{1} + x_{2} \\geq 3$, we found that this hypothesis would predict y=1 whenever x1+x2 is greater than or equal to 3.\n",
    "- Let's see what that means on the figure, if I write down the equation, $ x_{1} + x_{2} = 3$, , this defines\n",
    "the equation of a straight line and if I draw what that straight, it gives me the following line which passes through 3 and\n",
    "3 on the x1 and the x2 axis.\n",
    "- So the part of the input space, we will have 2 regions: \n",
    "    - y = 1 region, where $ x_{1} + x_{2} \\geq 3$\n",
    "    - y = 0 region, where $ x_{1} + x_{2} \\leq 3$\n",
    "- The line $ x_{1} + x_{2} = 3$, where $h_{\\theta}(x) = 0.5$ exactly, that seperates 2 regions is called the **decision boundary**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic09.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/WuL1H/decision-boundary) 9:40*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic10.png\">\n",
    "<img src=\"images/lec6_pic11.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/WuL1H/decision-boundary) 9:47*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a more complex example. Suppose a hypothesis is  \n",
    "$$\\large h_{\\theta}(x) = g(\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2}) + \\theta_{3}x_{1}^{2} + \\theta_{4}x_{2}^{2})$$\n",
    "- We choose $\\theta_{0}= - 3 $, $\\theta_{1}= 0 $, $\\theta_{2}= 0$,  $\\theta_{3}= 1$,  $\\theta_{4}= 1 $. This means that my parameter vector is going to be $ \\theta = \n",
    "\\begin{bmatrix}\n",
    "    -1\\\\\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "- This means that my hypothesis will predict that\n",
    "$$ \\text{Predict \"y = 1\" if } -1 + x_{1}^{2} + x_{2}^{2} \\geq 0  \\text{ or } x_{1}^{2} + x_{2}^{2} \\geq 1$$\n",
    "- So what does this decision boundary look like? If you were to plot the curve for $ x_{1}^{2} + x_{2}^{2} \\geq 1 $, that is the equation for circle of radius one, centered around the origin.\n",
    "- So that is my decision boundary. And everything outside the circle, I'm going to predict as y=1, and inside the circle is where I'll predict y = 0.\n",
    "- So by adding these more complex, or these polynomial terms to my features, I can get more complex decision boundaries that don't just try to separate the positive and negative examples in a straight line but instead a decision boundary that's a circle.\n",
    "- Once again, the decision boundary is a property, not of the training set, but of the hypothesis under the parameters. So as long as we're given my parameter vector theta, that defines the decision boundary, which is the circle. But the training set is not what we use to define the decision boundary. The training set may be used to fit the parameters theta.\n",
    "- So can we come up with even more complex decision boundaries then this? ? If I have even higher polynomial terms, then it's possible to show that you can get even more complex decision boundaries and the regression can be used to find decision boundaries. For example: an ellipse shape decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic12.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/WuL1H/decision-boundary) 14:24*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
