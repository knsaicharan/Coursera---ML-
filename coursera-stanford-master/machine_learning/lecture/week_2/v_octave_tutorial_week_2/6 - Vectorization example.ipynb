{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Table of Contents\n",
    "* [1) Using libraries](#1%29-Using-libraries)\t\n",
    "* [2) Vectorization example](#2%29-Vectorization-example)\n",
    "\t* [1) Octave example](#1%29-Octave-example)\n",
    "    * [2) C++ example](#2%29-C%2B%2B-example)\n",
    "    * [3) Gradient descent](#3%29-Gradient-descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Using libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you using Octave, MATLAB, Python, R, Java, C++, all of these languages have either built into them or have regularly and easily accessible difference in numerical linear algebra libraries. The libraries usually very well written, highly optimized, often sort of developed by people that have PhDs in numerical computing or they're really specialized in numerical computing.\n",
    "\n",
    "So when you're implementing machine learning algorithms, it is better to take advantage of these numerical linear algebra libraries, and make some routine calls to them rather than write code yourself to do things that these libraries could be doing.\n",
    "\n",
    "With this concept, you will get code that is more efficient, so you just run more quickly and take better advantage of any parallel hardware your computer may have and so on. And second, it also means that you end up with less code that you need to write, so it's a simpler implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Vectorization example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our usual hypothesis for linear regression as an example:\n",
    "$$\\large h_{\\theta}(x) = \\sum_{j=0}^n(\\theta_{j}x_{j}) = \\theta^{T}x$$\n",
    "\n",
    "There are 2 ways you can implement this hypothesis:\n",
    "- **Unvectorized implementation**: because the index in octave (matlab) starts at 1, you first need to initialize **prediction = 0.0**. Then to compute the sum from j = 0 to j = n, you need to have a **for loop for j = 1 through n+1**, prediction gets incremented by theta(j) * x(j). This is an unvectorized implementation in that we have for loop that is summing up the n elements of the sum.\n",
    "- **Vectorized implementation**: **prediction = theta' * x**. Instead of writing multiple lines of code, you just need 1 line. This line of code will use Octaves highly optimized numerical linear algebra routines to compute this inner product between the two vectors, theta and X, and not only is the vectorized implementation simpler, it will also run much more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Octave example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec5_pic01.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/WnQWH/vectorization) 4:13*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2) C++ example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec5_pic02.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/WnQWH/vectorization) 5:31*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our update rule for a gradient descent of a linear regression. We update theta j using this rule for all values of j = 0, 1, 2, and so on, and these should be simultaneous updates. So, let's see if we can come up with a vectorizing notation of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec5_pic05.png\">\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/WnQWH/vectorization) 6:10*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have three equations below of $\\theta_{0}, \\theta_{1}, \\theta_{2}$, then one way to implement these three lines of code is to have a for loop that says for j = 0, 1 through 2 to update theta j.\n",
    "\n",
    "Instead of using the for-loop, you should come up with a vectorized implementation, to compress 3 lines of code into 1 line. Here's the idea:\n",
    "- Thinking of $\\theta$ as a vector, I am gonna update theta as theta minus alpha times some other vector delta\n",
    "$$\\large\\theta := \\theta - \\alpha\\delta$$\n",
    "where $\\large\\delta = \\dfrac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$\n",
    "- I am going to treat:\n",
    "    - theta ($\\large\\theta$) as a (n+1) dimensional vector ($\\in \\mathbf{R}^{n+1}$), theta gets updated as a vector $\\mathbf{R}^{n+1}$\n",
    "    - alpha ($\\large\\alpha$) is a real number\n",
    "    - delta ($\\large\\delta$) is a vector.\n",
    "    - subtraction operation (-), that is a vector subtraction\n",
    "- We have vector\n",
    "$\n",
    "\\large\\delta = \n",
    "\\begin{bmatrix}\n",
    "    \\delta_{0} \\\\\n",
    "    \\delta_{1} \\\\\n",
    "    \\delta_{2} \n",
    "\\end{bmatrix}\n",
    "$, and $\\large \\delta_{0} = \\dfrac{1}{m}\\sum(h_{\\theta}(x^{(i)}) - y^{(i)})x_{0}^{(i)}$\n",
    "- Based on $\\large\\delta = \\dfrac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$ function, we have:\n",
    "    - $(h_{\\theta}(x^{(i)}) - y^{(i)})$: is a real number ($\\mathbf{R}$)\n",
    "    - $x^{(i)}$: is a vector ($\\mathbf{R}^{n+1}$), where \n",
    "    $x^{(i)} = \n",
    "    \\begin{bmatrix}\n",
    "    x_{0}^{(i)} \\\\\n",
    "    x_{1}^{(i)} \\\\\n",
    "    x_{2}^{(i)}\n",
    "    \\end{bmatrix}$\n",
    "- The summation is that this term $\\large (h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)} = (h_{\\theta}(x^{(1)}) - y^{(1)})x^{(1)} + (h_{\\theta}(x^{(2)}) - y^{(2)})x^{(2)} + ...$\n",
    "- The meaning of the term above is what we get from the previous quiz. In order to vectorize the code below:\n",
    "\\begin{eqnarray}\n",
    "u(j) &=& 2v(j) + 5w(j) \\text{   (for all j)} \\\\\n",
    " &=& 2v + 5w \\text{   (we just need to do this)}\n",
    "\\end{eqnarray}\n",
    "    - We are saying that vector u = 2(vector v) + 5(vector w)\n",
    "- Similar $(h_{\\theta}(x^{(1)}) - y^{(1)})$ is a number times $x^{(1)}$, and $(h_{\\theta}(x^{(2)}) - y^{(2)})$ is another number times $x^{(2)}$.\n",
    "- So this whole quantity $(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$ indicates that $\\delta$ is just some vector.\n",
    "- Concretly, if n = 2, the three elements of $\\delta$ correspond to the second part in the function (example: $(h_{\\theta}(x^{(i)}) - y^{(i)})x_{0}^{(i)}$) of three elemments of $\\theta$. Which is why when you update theta according to $\\theta := \\theta - \\alpha\\delta$, we end up carrying exactly the same simultaneous updates as the update rules that we have up top.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec5_pic03.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/WnQWH/vectorization) 12:50*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec5_pic04.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/WnQWH/vectorization) 13:06*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, if you are implementing linear regression using more than one or two features, so sometimes we use linear regression with 10's or 100's or 1,000's of features. If you use the vectorized implementation of linear regression, you'll see that will run much faster compare to the old for loop that was updating theta zero, then theta one, then theta two ...\n",
    "\n",
    "So, using a vectorized implementation, you should be able to get a much more efficient implementation of linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
