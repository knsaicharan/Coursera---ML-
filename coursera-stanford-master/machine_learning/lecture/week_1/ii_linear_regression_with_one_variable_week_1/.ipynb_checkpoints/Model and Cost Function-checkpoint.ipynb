{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Table of Contents\n",
    "* [Model Representation](#Model-Representation)\n",
    "\t* [Notation](#Notation)\n",
    "* [Cost Function](#Cost-Function)\n",
    "\t* [1) Hypothesis](#1%29-Hypothesis)\n",
    "\t* [2) Idea](#2%29-Idea)\n",
    "\t\t* [Why 1/2m instead of 1/m](#Why-1/2m-instead-of-1/m)\n",
    "\t* [3) Cost Function](#3%29-Cost-Function)\n",
    "* [Cost Function - Intuition I](#Cost-Function---Intuition-I)\n",
    "\t* [1) Simplied Cost Function](#1%29-Simplied-Cost-Function)\n",
    "\t* [2) Example](#2%29-Example)\n",
    "\t\t* [a) theta1 = 1](#a%29-theta1-=-1)\n",
    "\t\t* [b) theta1 = 0.5](#b%29-theta1-=-0.5)\n",
    "\t\t* [c )theta1 = 0](#c-%29theta1-=-0)\n",
    "* [Cost Function - Intuition II](#Cost-Function---Intuition-II)\n",
    "\t* [1) Plot 2 parameters Cost Function](#1%29-Plot-2-parameters-Cost-Function)\n",
    "\t* [2) Contour plots](#2%29-Contour-plots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_1.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/db3jS/model-representation) 4:5*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_4.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/db3jS/model-representation) 4:38*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cost function will let us figure out how to fit the best possible straight line to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1) Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_2.png\">\n",
    "<img src=\"images/pic_3.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/rkTp3/cost-function) 0:32*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_5.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/rkTp3/cost-function) 2:13*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The idea is we get to choose our parameters $\\large \\theta_{0}$, $\\large \\theta_{1}$ so that $\\large h(x)$, meaning the value we predict on equals x, that this is at least close to the values y for the examples in our training set, for our training examples**\n",
    "\n",
    "- We want to solve the minimization problems: minimize $\\large \\theta_{0} \\theta_{1}$ so that the square difference between the output of the hypothesis and the actual price $\\large (h_{\\theta}(x) - y)^{2}$ to be small.\n",
    "- Remember $\\large (x^{(i)}, y^{(i)})$ to repesent the $\\large i^{th}$ training example, and $\\large m$ is the size of the training set.\n",
    "- Then we can sum over the training set, to minimize the sum from $\\large i^{th}\\longrightarrow m$ of the difference of this squared error:    \n",
    "$$\\large \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^{2}$$\n",
    "- And to make the math becomes easier, we can try to minimize the average of m by putting $\\large \\frac{1}{2}$ (a constant) in front of the function. Minimizing one half of something should give you the same values of the process $\\large \\theta_{0} \\theta_{1}$ as minimizing that function.\n",
    "\n",
    "$$\\large \\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^{2}$$\n",
    "\n",
    "** Conclusion ** : \n",
    "\n",
    "- Given: \n",
    "$\\large h_{\\theta}(x{i}) = \\theta_{0} + \\theta_{1}x^{(i)}$ \n",
    "\n",
    "- Find \n",
    "$$\\large \\theta_{0}, \\theta_{1}$$\n",
    "\n",
    "so that \n",
    "\n",
    "$$\\large \\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^{2}$$ is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why 1/2m instead of 1/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On other website you will see the error function is only 1/N, so why 1/2M: http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/\n",
    "- Explaination:\n",
    "    - https://www.coursera.org/learn/machine-learning/lecture/kCvQc/gradient-descent-for-linear-regression\n",
    "    - Since we are using the quadratic function for the cost. The factor 1/2 is only here in anticipation to the fact that we will use the gradient of that cost function. Because when you take the derivative of the squared error, you have a factor 2 that appears and so it is cancelled out by the 1/2 factor. But mathematically you could have any strictly positive real number scaling your cost function and it will not change the value of the optimal parameters that minimize the function that is why in some textbooks you may find the definition without the 1/2 factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **_Cost funtion_** is also called **_squared error function_** or **_squared error cost function_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_6.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/rkTp3/cost-function) 7:47*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function - Intuition I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Simplied Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_7.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/N09c6/cost-function-intuition-i) 1:12*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given 3 points of y: (1,1), (2,2), (3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) theta1 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we have a function of x where  $\\large \\theta_{1} = 1$, then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_8.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/N09c6/cost-function-intuition-i) 4:52*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, when $\\theta_{1} = 1$, the cost funtion will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "\\large J(\\theta_{1}) &=& \\large\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)}) - y^{(i)})^{2} \\nonumber \\\\\n",
    "\\large               &=& \\large\\frac{1}{2m}\\sum_{i=1}^m(\\theta_{1}x^{(i)} - y^{(i)})^{2} \\nonumber \\\\\n",
    "\\large               &=& \\large\\frac{1}{2m}(0^2 + 0^2 + 0^2) \\nonumber \\\\\n",
    "\\large               &=& \\large0^2 \\nonumber \\\\\n",
    "\\large J(\\theta_{1}) &=& \\large0 \\nonumber  \\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) theta1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_9.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/N09c6/cost-function-intuition-i) 6:50*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, when $\\theta_{1} = 1$, the cost funtion will be:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\large J(\\theta_{1}) &=& \\large\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)}) - y^{(i)})^{2} \\nonumber \\\\\n",
    "\\large               &=& \\large\\frac{1}{2m}\\sum_{i=1}^m(\\theta_{1}x^{(i)} - y^{(i)})^{2} \\nonumber \\\\\n",
    "\\large               &=& \\large\\frac{1}{2m}[(0.5 - 1)^2 + (1 - 2)^2 + (1.5 - 3)^2] \\nonumber \\\\\n",
    "\\large               &=& \\large\\frac{1}{2 * 3}(3.5) \\nonumber \\\\\n",
    "\\large               &=& \\large\\frac{3.5}{6} \\nonumber \\\\\n",
    "\\large J(\\theta_{1}) &\\approx& \\large 0.58 \\nonumber  \\\\\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c )theta1 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_10.png\">\n",
    "<img src=\"images/pic_11.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/N09c6/cost-function-intuition-i) 6:54*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "\\large J(\\theta_{1}) &=& \\large\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)}) - y^{(i)})^{2} \\nonumber \\\\\n",
    "\\large               &=& \\large\\frac{1}{2m}\\sum_{i=1}^m(\\theta_{1}x^{(i)} - y^{(i)})^{2} \\nonumber \\\\\n",
    "\\large               &=& \\large\\frac{1}{2m}[(0 - 1)^2 + (0 - 2)^2 + (0 - 3)^2] \\nonumber \\\\\n",
    "\\large               &=& \\large\\frac{1}{2 * 3}(14) \\nonumber \\\\\n",
    "\\large               &=& \\large\\frac{14}{6} \\nonumber \\\\\n",
    "\\large J(\\theta_{1}) &\\approx& \\large 2.3 \\nonumber  \\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_12.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/N09c6/cost-function-intuition-i) 9:14*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each value of theta one corresponds to a different hypothesis, or to a different straight line fit on the left. And for each value of theta one, we could then derive a different value of j of theta one. And for example, you know, theta one=1, corresponded to this straight line straight through the data.\n",
    "- Looking at this curve, the value that minimizes j of theta one is, you know, $\\large \\theta_{1} = 1$. And that is indeed the best possible straight line fit through our data, by setting theta one equals one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function - Intuition II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Plot 2 parameters Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we only have parameter $\\theta_{1}$, we have a cost function that looks like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_13.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/nwpe2/cost-function-intuition-ii) 1:52*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But now we have two parameters, theta zero, and theta one, and so the plot gets a little more complicated. It turns out that when we have only one parameter, that the parts we drew had this sort of bow shaped function. Now, when we have two parameters, it turns out the cost function also has a similar sort of bow shape. And, in fact, depending on your training set, you might get a cost function that maybe looks something like this. So, this is a 3-D surface plot, where the axes are labeled theta zero and theta one. So as you vary theta zero and theta one, the two parameters, you get different values of the cost function J (theta zero, theta one) and the height of this surface above a particular point of theta zero, theta one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_14.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/nwpe2/cost-function-intuition-ii) 2:16*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Contour plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_15.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/nwpe2/cost-function-intuition-ii) 3:41*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic_16.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/nwpe2/cost-function-intuition-ii) 7:30*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
