{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classificaion: One-vs-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll talk about how to get logistic regression to work for multiclass classification problems. And in particular I want to tell you about an algorithm called **one-versus-all classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic31.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/68Pol/multiclass-classification-one-vs-all) 1:24*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a **multi-class classification** problem our data sets may look like this where here I'm using three different symbols to represent our three classes. We already know how to do **binary classification using a regression**. We know how to you know maybe fit a straight line to set for the positive and negative classes. You see an idea called **one-vs-all classification**. We can then take this and make it work for multi-class classification as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic32.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/68Pol/multiclass-classification-one-vs-all) 2:00*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's how a one-vs-all classification works**. And this is also sometimes called one-vs-rest.\n",
    "\n",
    "Let's say we have a training set like that shown on the left, where we have three classes of y = 1, we denote that with a triangle, if y = 2, the square, and if y = 3, then the cross. What we're going to do is take our training set and turn this into three separate binary classification problems.\n",
    "\n",
    "**So let's start with class one which is the triangle**. \n",
    "- We're gonna essentially create a new sort of fake training set where classes two and three get assigned to the negative class. And class one gets assigned to the positive class. I'm going to call $\\large h_{\\theta}^{(1)}(x)$ where here the triangles are the positive examples and the circles are the negative examples. \n",
    "- So think of the triangles being assigned the value of one and the circles assigned the value of zero. And we're just going to train a standard logistic regression classifier and maybe that will give us a decision boundary that separates the two. So $\\large h_{\\theta}^{(1)}(x)$ with this superscript one (1) here stands for class one, so we're doing this for the triangles of class one.\n",
    "\n",
    "**We are doing the same thing for class 2 and 3**. \n",
    "\n",
    "So, for i = 1, 2, 3, we'll fit a classifier $\\large h_{\\theta}^{(i)}(x)$. Thus trying to estimate what is the probability that y is equal to class i, given x and parametrized by theta.\n",
    "\n",
    "So in the first instance for this first triangle class, this classifier was learning to recognize the triangles. So it's thinking of the triangles as a positive clause, so $\\large h_{\\theta}^{(1)}(x)$ is essentially trying to estimate what is the probability that the y is equal to one, given that x is parametrized by theta. \n",
    "\n",
    "And similarly, this is treating the square class as a positive class and so it's trying to estimate the probability that y = 2 and so on. So we now have three classifiers, each of which was trained to recognize one of the three classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic33.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/68Pol/multiclass-classification-one-vs-all) 5:16*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we've done is we want to train a logistic regression classifier $\\large h_{\\theta}^{(i)}(x)$ for each class i to predict\n",
    "the probability that y is equal to i.**\n",
    "\n",
    "Finally to make a prediction, when we're given a new input x, and we want to make a prediction. \n",
    "\n",
    "What we do is we just run all three of our classifiers on the input x and we then pick the class i that maximizes the three. So we just basically pick the classifier, I think whichever one of the three classifiers is most confident and so the most enthusiastically says that it thinks it has the right clause. So whichever value of i gives us the highest probability we then predict y to be that value. So that's it for **multi-class classification and one-vs-all method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic34.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/68Pol/multiclass-classification-one-vs-all) 5:35*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic35.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/68Pol/multiclass-classification-one-vs-all) 6:01*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
