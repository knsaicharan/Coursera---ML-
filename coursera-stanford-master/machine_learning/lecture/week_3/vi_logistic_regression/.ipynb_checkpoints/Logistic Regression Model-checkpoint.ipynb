{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1) Cost Function](#1%29-Cost-Function)\n",
    "    * [1) Parameter theta](#1%29-Parameter-theta)\n",
    "\t* [2) Linear regression cost function](#2%29-Linear-regression-cost-function)\n",
    "    * [3) Logistic regression cost function](#3%29-Logistic-regression-cost-function)\n",
    "* [2) Simplified cost function and gradient descent](#2%29-Simplified-cost-function-and-gradient-descent)\n",
    "* [3) Advanced Optimization](#3%29-Advanced-Optimization)\n",
    "\t* [1) Gradient descent](#1%29-Gradient-descent)\n",
    "\t* [2) Advanced algorithms](#2%29-Advanced-algorithms)\n",
    "    * [3) Example](#3%29-Example)\n",
    "    * [4) Apply to logistic regression](#4%29-Apply-to-logistic-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Parameter theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic13.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function) 00:10*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's the supervised learning problem of fitting logistic regression model.** \n",
    "\n",
    "We have a training set of m training examples and as usual, each of our examples is represented by a (n + 1) dimensional, and we have $x_{0} = 1$. First feature or a zero feature is always equal to one. And because this is a computation problem, our training set has the property that every label y is either 0 or 1. This is a hypothesis, and the parameters of a hypothesis is this theta over here. And the question that I want to talk about is given this training set, how do we choose, or how do we fit the parameter's theta? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic14.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function) 01:00*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Linear regression cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back when we were developing the **linear regression model**, we used the following **cost function**. I've written this slightly differently where instead of $\\dfrac{1}{2m}$, I've taken a $\\dfrac{1}{2}$ and put it inside the summation instead. Now I want to use an alternative way of writing out this cost function. \n",
    "\n",
    "I will define:\n",
    "$$\\large cost(h_{\\theta}(x^{(i)}, y) = \\dfrac{1}{2}\\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^{2}$$\n",
    "\n",
    "The cost function above works fine for linear regression. But it turns out that if we use this particular cost function, this would be a **non-convex function of the parameter's data**\n",
    "\n",
    "When you plugin $h_{\\theta}(x) = \\dfrac{1}{1 + e^{-\\theta^{T}x}}$ into the cost function, and then take this cost function and\n",
    "plug it in $J_{\\theta}$, and then plot what $J_{\\theta}$ looks like. You find that the $J_{\\theta}$ can look\n",
    "like a function with many local optima, And the formal term for this is a **non-convex function**.\n",
    "\n",
    "If you were to run gradient descent on this sort of function, it is not guaranteed to converge to the global minimum. Whereas in contrast what we would like is to have a cost function $J_{\\theta}$ that is convex, that is a **single bow-shaped\n",
    "function** so that we would be guaranteed that would converge to the global minimum.\n",
    "\n",
    "And the problem with using this cost function is that because of this very non-linear function $h_{\\theta}(x) = \\dfrac{1}{1 + e^{-\\theta^{T}x}}$. So what we'd like to do is to come up with a different cost function, that is convex, and so that we can apply a great algorithm, like gradient descent and be guaranteed to find the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic15.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function) 04:25*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Logistic regression cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Cost(h_{\\theta}(x), y) =\n",
    "\\left\\{\n",
    "\t\\begin{array}\\\\\n",
    "\t\t- \\log (h_{\\theta}(x) & \\mbox{if } y = 1 \\\\\n",
    "\t\t- \\log (1 - h_{\\theta}(x)) & \\mbox{if } y = 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a pretty complicated function, but let's plot this function to gain some intuition about what it's doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic16.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function) 06:30*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this cost function has a few **interesting and desirable properties**. \n",
    "- First, you notice that if y = 1 and h(x) = 1, in other words, if the hypothesis exactly predicts h = 1 and y is exactly equal to what it predicted, then the cost = 0. That corresponds to the curve doesn't actually flatten out, or the point in the x-axis.\n",
    "- But if h(x) = 1 and the cost = 0, so as h(x) approaches 0, so as the output of a hypothesis approaches 0, the cost blows up and it goes to infinity. What this does is this captures the intuition that if a hypothesis of 0, that's like saying a hypothesis saying the chance of y equals 1 is equal to 0.\n",
    "\n",
    "**Example**: It's kinda like we are going to our medical patients and saying the probability that you have a malignant tumor, the probability that y=1, is zero. So, it's like absolutely impossible that your tumor is malignant. But if it turns out that the tumor, the patient's tumor, actually is malignant. So it's absolutely impossible for it to be malignant. But if we told them this with that level of certainty and we turn out to be wrong, then we penalize the learning algorithm by a very, very large cost. And that's captured by having this cost go to infinity if y = 1 and h(x) approaches 0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic17.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function) 08:20*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot the cost function for the case of y=0**. \n",
    "\n",
    "What this cost function does is that it goes up or it goes to a positive infinity as h(x) goes to 1, and this catches the intuition that if a hypothesis predicted that your h of x is equal to 1 with certainty, with probably ones, absolutely gonna be y equals 1.\n",
    "\n",
    "But if y turns out to be equal to 0, then it makes sense to make the hypothesis, or the learning algorithm up here a very large cost.\n",
    "\n",
    "And conversely, if h of x is equal to 0 and y equals 0, then the hypothesis melted. The protected y of z is equal to 0, and it turns out y is equal to 0, so at this point, the cost function is going to be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic18.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function) 10:20*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic19.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function) 10:40*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2) Simplified cost function and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our cost function for logistic regression.\n",
    "\n",
    "$$\n",
    "\\large\n",
    "Cost(h_{\\theta}(x), y) =\n",
    "\\left\\{\n",
    "\t\\begin{array}\\\\\n",
    "\t\t- \\log (h_{\\theta}(x) & \\mbox{if } y = 1 \\\\\n",
    "\t\t- \\log (1 - h_{\\theta}(x)) & \\mbox{if } y = 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "In a classification problems in our training sets, our training set y is always equal to zero or one. Because y is either zero or one, we'll be able to come up with a simpler way to write this cost function. I'm going to show you a way to take these two lines above and compress them into one equation.\n",
    "\n",
    "\n",
    "$\\large \\text{Cost}(h_{\\theta}(x), y) = -y\\log (h_{\\theta}(x) - (1-y)\\log (1 - h_{\\theta}(x))$\n",
    "- If y = 1: $\\text{Cost}(h_{\\theta}(x), y) = - \\log (h_{\\theta}(x)$\n",
    "- If y = 0: $\\text{Cost}(h_{\\theta}(x), y) = - \\log (1 - h_{\\theta}(x))$\n",
    "\n",
    "So this shows that this definition for the cost is just a more compact way of taking both of these expressions, the cases y =1 and y = 0, and writing them in a more convenient form with just one line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic20.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent) 3:50*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore write all our cost functions for logistic regression as follows.\n",
    "\n",
    "$$\\large J(\\theta) = \\dfrac{1}{m}\\sum_{i=1}^m \\text{Cost}(h_{\\theta}(x^{(i)}, y^{(i)}) $$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\large J(\\theta) &=& \\large\\dfrac{1}{m}\\sum_{i=1}^m \\text{Cost}(h_{\\theta}(x^{(i)}, y^{(i)})  \\nonumber \\\\\n",
    "\\large            &=& \\large -\\dfrac{1}{m}[\\sum_{i=1}^m y^{(i)}\\log (h_{\\theta}(x^{(i)}) + (1 - y^{(i)})\\log (1 - h_{\\theta}(x^{(i)})] \\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "This cost function above can be derived from statistics using the principle of maximum likelihood estimation. Which is an idea in statistics for how to efficiently find parameters' data for different models. And it also has a nice property that it is convex. So this is the cost function that essentially everyone uses when fitting logistic regression models.\n",
    "\n",
    "Given this cost function, in order to fit the parameters, what we're going to do then is try to find the parameters theta that minimize $\\large J_{\\theta}$. So if we try to minimize this, this would give us some set of parameters theta.\n",
    "\n",
    "Finally, if we're given a new example with some set of features x, we can then take the thetas that we fit to our training set and output our prediction. The output of my hypothesis I'm going to interpret as the probability that y is equal to one. And given the input x and parameterized by theta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic21.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent) 5:51*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we're going to minimize the cost function is using **gradient descent**. If we want to to minimize our cost function $\\large J_{\\theta}$, we need to repeatedly update each parameter as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic22.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent) 6:48*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you have n features, you would have a parameter vector $\\large \\theta$, , which with parameters $\\large \\theta_{0}, \\theta_{1}, \\theta_{2}, ... , \\theta_{n}$. And you will use this update to simultaneously update all of your values of theta.\n",
    "\n",
    "Now, if you take this update rule and compare it to what we were doing for linear regression. You might be surprised to realize that this equation was exactly what we had for linear regression. The only difference is that the definition for this hypothesis has changed.\n",
    "- For linear regression: $\\large h_{\\theta}(x) = \\theta^{T}x$\n",
    "- For logistic regression: $\\large h_{\\theta}(x) = \\dfrac{1}{1 + e^{-\\theta^{T}x}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic23.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent) 8:38*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an earlier video, when we were talking about gradient descent for linear regression, we had talked about how to monitor a gradient descent to make sure that it is converging. I usually apply that same\n",
    "method to logistic regression, too to monitor a gradient descent, to make sure it's converging correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic24.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent) 8:44*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing logistic regression with gradient descent, we have all of these ifferent parameter values, theta zero down to theta n, that we need to update using this expression. And one thing we could do is have a for-loop.\n",
    "\n",
    "But of course rather than using a for-loop, ideally we would also usea vectorrize implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic25.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent) 9:30*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3) Advanced Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to tell you about some **advanced optimization algorithms** and **some advanced optimization concepts**. Using some of these ideas, we'll be able to get logistic regression to run much more quickly than it's possible with gradient descent. And this will also let the algorithms scale much better to very large machine learning problems, such as if we had a very large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's an alternative view of what gradient descent is doing.** \n",
    "\n",
    "We have some cost function J and we want to minimize it. So what we need to is, we need to write code that can take as input the parameters theta and they can compute two things: $\\large J_{\\theta}$ and these partial derivative terms $\\large\\frac{\\partial}{\\partial \\theta_{j}}J_{(\\theta)}$ for, you know, J = 0, 1 up to N.\n",
    "\n",
    "So given the code that we wrote to compute these partial derivatives, gradient descent plugs in here and uses that to update our parameters theta. So another way of thinking about gradient descent is that we need to supply code to compute $\\large J_{\\theta}$ and these derivatives, and then these get plugged into gradient descents, which can then try to minimize the function for us.\n",
    "\n",
    "For gradient descent, I guess technically you don't actually need code to compute the cost function J of theta. You only need code to compute the derivative terms. But if you think of your code as also monitoring convergence of some such, we'll just think of ourselves as providing code to compute both the cost function and the derivative terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic26.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/licwf/advanced-optimization) 01:48*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Advanced algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, having written code to compute these two things, one algorithm we can use is gradient descent. But gradient descent isn't the only algorithm we can use. And there are other algorithms, more advanced, more sophisticated ones, that, if we only provide them a way to compute these two things, then these are different approaches to optimize the cost function for us.\n",
    "- Conjugate gradient\n",
    "- BFGS\n",
    "- L-BFGS\n",
    "\n",
    "These 3 are examples of more sophisticated optimization algorithms that need a way to compute J of theta, and  compute the derivatives, and can then use more sophisticated strategies than gradient descent to minimize the cost function.\n",
    "\n",
    "**Advantages of the 3 advanced algorithm:**\n",
    "- With any of this algorithms, you usually do not need to manually pick the learning rate alpha. You can think of these algorithms as having a clever **inter-loop** called a **line search algorithm** that automatically tries out different values for the learning rate alpha and automatically picks a good learning rate alpha so that it can even pick a different learning rate for every iteration.\n",
    "- They often end up converging much faster than gradient descent.\n",
    "\n",
    "**Disadvantages**\n",
    "- They're quite a lot more complex than gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic27.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/licwf/advanced-optimization) 04:20*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So now let's explain how to use these algorithms, I'm going to do so with an example.**\n",
    "\n",
    "Let's say that you have a problem with two parameters equals theta zero and theta one. If you want to minimize $\\large J_{\\theta}$ as a function of theta, The value that minimizes it is going to be $\\theta_{1}$ = 5, $\\theta_{2}$ = 5. Because we want to partial derivative of $\\theta$ = 0. \n",
    "\n",
    "**So if you want to apply one of the advanced optimization algorithms to minimize cost function J**\n",
    "\n",
    "- If we didn't know the minimum was at 5, 5, and you want to have a cost function using something like gradient descent but preferably more advanced than gradient descent, what you would do is implement an octave function like this.\n",
    "\n",
    "```octave\n",
    "% This is the cost function theta that return 2 arguments\n",
    "function [jVal, gradient] = costFunction(theta)\n",
    "\n",
    "% first argument jVal: is how we would compute the cost function J\n",
    "jVal = (theta(1)-5)^2 +  (theta(2)-5)^2;\n",
    "\n",
    "% second argument gradient: a (2 x 1) vector\n",
    "gradient = zeros(2,1);\n",
    "\n",
    "% the 2 elements of the gradient vector correspond to the two partial derivative terms \n",
    "gradient(1) = 2*(theta(1)-5);\n",
    "gradient(2) = 2*(theta(2)-5);\n",
    "```\n",
    "\n",
    "- Having implemented this cost function, you can then call the advanced optimization function called the **fminunc** - it stands for function minimization unconstrained in Octave. And the way you call this is as follows:\n",
    "\n",
    "```octave\n",
    "% This is a options as a data structure that stores the options you want\n",
    "% ‘GradObj’, ‘on’: this sets the gradient objective parameter to on.It just means you are indeed going to provide a gradient to this algorithm\n",
    "% ‘MaxIter’, ‘100’: maximum number of iterations = 100\n",
    "options = optimset(‘GradObj’, ‘on’, ‘MaxIter’, ‘100’);\n",
    "\n",
    "% give it an initial guess for theta, a (2 x 1) vector\n",
    "initialTheta = zeros(2,1);\n",
    "\n",
    "% call fminunc function\n",
    "% @costFunction: a pointer to the costFunction that we just defined in the code block above.\n",
    "[optTheta, functionVal, exitFlag] ...\n",
    " = fminunc(@costFunction, initialTheta, options); \n",
    "```\n",
    "\n",
    "This will then attempt to use the sort of advanced optimization algorithms. Like gradient descent on steroids. To try to find the optimal value of theta for you.\n",
    "\n",
    "**Notice**:\n",
    "- For Octave implementation, this value of theta (parameter of theta) initialTheta must be in $\\large \\theta \\in R^{d}$ for $\\large d \\geq 2$\n",
    "- So if theta is just a real number, if it is not at least a two-dimensional vector or some higher than two-dimensional vector, this fminunc may not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic28.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/licwf/advanced-optimization) 10:56*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Apply to logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we have a **parameter vector theta**, which comprises these parameters theta 0 through theta n. Because Octave uses index starting from 1, so theta 0 is actually written theta 1 in octave, and theta n will be theta n+1.\n",
    "\n",
    "So what we need to do then is write a cost function that captures the cost function for logistic regression. Concretely, the cost function needs to return **jval** [code to compute $J{\\theta}$], and we also need to give it the **gradient**. So, gradient 1 is going to be some code to compute the partial derivative in respect to theta 0, the next partial derivative respect to theta 1 and so on. Once again, this is gradient 1, gradient 2 and so on, rather than gradient 0, gradient 1 because octave indexes is vectors starting from one rather than from zero.\n",
    "\n",
    "But the main concept here is write a function that returns the cost function and returns the gradient. And so in order to apply this to logistic regression or even to linear regression, if you want to use these optimization algorithms for linear regression. What you need to do is plug in the appropriate code to compute these things over here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic29.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/licwf/advanced-optimization) 13:08*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec6_pic30.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/licwf/advanced-optimization) 13:14*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
