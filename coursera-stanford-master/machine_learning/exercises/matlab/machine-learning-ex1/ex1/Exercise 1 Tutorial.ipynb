{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1) ex1.m](#1%29-ex1.m)\n",
    "* [2) Solution of ex1.m](#2%29-Solution-of-ex1.m)    \n",
    "    * [1) warmUpExercise.m](#1%29-warmUpExercise.m)\n",
    "\t* [2) plotData.m](#2%29-plotData.m)\n",
    "    * [3) computeCost.m](#3%29-computeCost.m)\n",
    "    * [4) gradientDescent.m](#4%29-gradientDescent.m)\n",
    "* [3) ex1_multi.m](#3%29-ex1_multi.m)   \n",
    "* [4) Solution of ex1_multi.m](#4%29-Solution-of-ex1_multi.m)\n",
    "    * [1) featureNormalize.m](#1%29-featureNormalize.m)\n",
    "\t* [2) computeCostMulti.m](#2%29-computeCostMulti.m)\n",
    "    * [3) gradientDescentMulti.m](#3%29-gradientDescentMulti.m)\n",
    "    * [4) normalEqn.m](#4%29-normalEqn.m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exercise 1 Tutorial.ipynb\n",
      "computeCost.m\n",
      "computeCostMulti.m\n",
      "ex1.m\n",
      "ex1_multi.m\n",
      "ex1data1.txt\n",
      "ex1data2.txt\n",
      "featureNormalize.m\n",
      "gradientDescent.m\n",
      "gradientDescentMulti.m\n",
      "lib\n",
      "normalEqn.m\n",
      "plotData.m\n",
      "submit.m\n",
      "token.mat\n",
      "warmUpExercise.m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) ex1.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load ex1.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 1: Linear Regression\n",
    "\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear exercise. You will need to complete the following functions \n",
    "%  in this exericse:\n",
    "%\n",
    "%     warmUpExercise.m\n",
    "%     plotData.m\n",
    "%     gradientDescent.m\n",
    "%     computeCost.m\n",
    "%     gradientDescentMulti.m\n",
    "%     computeCostMulti.m\n",
    "%     featureNormalize.m\n",
    "%     normalEqn.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "% x refers to the population size in 10,000s\n",
    "% y refers to the profit in $10,000s\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "clear ; close all; clc\n",
    "\n",
    "%% ==================== Part 1: Basic Function ====================\n",
    "% Complete warmUpExercise.m \n",
    "fprintf('Running warmUpExercise ... \\n');\n",
    "fprintf('5x5 Identity Matrix: \\n');\n",
    "warmUpExercise()\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "\n",
    "%% ======================= Part 2: Plotting =======================\n",
    "fprintf('Plotting Data ...\\n')\n",
    "data = load('ex1data1.txt');\n",
    "X = data(:, 1); y = data(:, 2);\n",
    "m = length(y); % number of training examples\n",
    "\n",
    "% Plot Data\n",
    "% Note: You have to complete the code in plotData.m\n",
    "plotData(X, y);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%% =================== Part 3: Gradient descent ===================\n",
    "fprintf('Running Gradient Descent ...\\n')\n",
    "\n",
    "X = [ones(m, 1), data(:,1)]; % Add a column of ones to x\n",
    "theta = zeros(2, 1); % initialize fitting parameters\n",
    "\n",
    "% Some gradient descent settings\n",
    "iterations = 1500;\n",
    "alpha = 0.01;\n",
    "\n",
    "% compute and display initial cost\n",
    "computeCost(X, y, theta)\n",
    "\n",
    "% run gradient descent\n",
    "theta = gradientDescent(X, y, theta, alpha, iterations);\n",
    "\n",
    "% print theta to screen\n",
    "fprintf('Theta found by gradient descent: ');\n",
    "fprintf('%f %f \\n', theta(1), theta(2));\n",
    "\n",
    "% Plot the linear fit\n",
    "hold on; % keep previous plot visible\n",
    "plot(X(:,2), X*theta, '-')\n",
    "legend('Training data', 'Linear regression')\n",
    "hold off % don't overlay any more plots on this figure\n",
    "\n",
    "% Predict values for population sizes of 35,000 and 70,000\n",
    "predict1 = [1, 3.5] *theta;\n",
    "fprintf('For population = 35,000, we predict a profit of %f\\n',...\n",
    "    predict1*10000);\n",
    "predict2 = [1, 7] * theta;\n",
    "fprintf('For population = 70,000, we predict a profit of %f\\n',...\n",
    "    predict2*10000);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%% ============= Part 4: Visualizing J(theta_0, theta_1) =============\n",
    "fprintf('Visualizing J(theta_0, theta_1) ...\\n')\n",
    "\n",
    "% Grid over which we will calculate J\n",
    "theta0_vals = linspace(-10, 10, 100);\n",
    "theta1_vals = linspace(-1, 4, 100);\n",
    "\n",
    "% initialize J_vals to a matrix of 0's\n",
    "J_vals = zeros(length(theta0_vals), length(theta1_vals));\n",
    "\n",
    "% Fill out J_vals\n",
    "for i = 1:length(theta0_vals)\n",
    "    for j = 1:length(theta1_vals)\n",
    "\t  t = [theta0_vals(i); theta1_vals(j)];    \n",
    "\t  J_vals(i,j) = computeCost(X, y, t);\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "% Because of the way meshgrids work in the surf command, we need to \n",
    "% transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals';\n",
    "% Surface plot\n",
    "figure;\n",
    "surf(theta0_vals, theta1_vals, J_vals)\n",
    "xlabel('\\theta_0'); ylabel('\\theta_1');\n",
    "\n",
    "% Contour plot\n",
    "figure;\n",
    "% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n",
    "contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))\n",
    "xlabel('\\theta_0'); ylabel('\\theta_1');\n",
    "hold on;\n",
    "plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Solution of ex1.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) warmUpExercise.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load warmUpExercise.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function A = warmUpExercise()\n",
    "%WARMUPEXERCISE Example function in octave\n",
    "%   A = WARMUPEXERCISE() is an example function that returns the 5x5 identity matrix\n",
    "\n",
    "A = [];\n",
    "% ============= YOUR CODE HERE ==============\n",
    "% Instructions: Return the 5x5 identity matrix \n",
    "%               In octave, we return values by defining which variables\n",
    "%               represent the return values (at the top of the file)\n",
    "%               and then set them accordingly. \n",
    "\n",
    "A = eye(5);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "% ===========================================\n",
    "\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) plotData.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load plotData.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function plotData(x, y)\n",
    "%PLOTDATA Plots the data points x and y into a new figure \n",
    "%   PLOTDATA(x,y) plots the data points and gives the figure axes labels of\n",
    "%   population and profit.\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Plot the training data into a figure using the \n",
    "%               \"figure\" and \"plot\" commands. Set the axes labels using\n",
    "%               the \"xlabel\" and \"ylabel\" commands. Assume the \n",
    "%               population and revenue data have been passed in\n",
    "%               as the x and y arguments of this function.\n",
    "%\n",
    "% Hint: You can use the 'rx' option with plot to have the markers\n",
    "%       appear as red crosses. Furthermore, you can make the\n",
    "%       markers larger by using plot(..., 'rx', 'MarkerSize', 10);\n",
    "\n",
    "figure; % open a new figure window\n",
    "\n",
    "plot(x, y, 'rx', 'MarkerSize', 10);       % Plot the data\n",
    "ylabel('Profit in $10,000s');             % Set the y??axis label\n",
    "xlabel('Population of City in 10,000s');  % Set the x??axis label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "% ============================================================\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) computeCost.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/machine-learning/module/vW94N/discussions/t35D1xn3EeWA7CIAC5WDNQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load computeCost.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function J = computeCost(X, y, theta)\n",
    "%COMPUTECOST Compute cost for linear regression\n",
    "%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the\n",
    "%   parameter for linear regression to fit the data points in X and y\n",
    "\n",
    "% Initialize some useful values\n",
    "m = length(y); % number of training examples\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "J = 0;\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Compute the cost of a particular choice of theta\n",
    "%               You should set J to the cost.\n",
    "\n",
    "predictions = X * theta; % predictions of hypothesis on all m examples\n",
    "sqrErrors = (predictions-y).^2; \t% squared errors\n",
    "J = 1/(2*m) * sum(sqrErrors);\n",
    "\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4) gradientDescent.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/machine-learning/discussions/-m2ng_KQEeSUBCIAC9QURQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is a tutorial on implementing gradientDescent() and gradientDescentMulti().**\n",
    "\n",
    "I use the vectorized method, hopefully you're comfortable with vector math. Using this method means you don't have to fuss with array indices, and your solution will automatically work for any number of features or training examples.\n",
    "\n",
    "What follows is a vectorized implementation of the gradient descent equation on the bottom of Page 5 in ex1.pdf.\n",
    "\n",
    "Reminder that 'm' is the number of training examples (the rows of X), and 'n' is the number of features (the columns of X). 'n' is also the size of the theta vector (n x 1).\n",
    "\n",
    "Perform all of these steps within the provided for-loop from 1 to the number of iterations. Note that the code template provides you this for-loop - you only have to complete the body of the for-loop. The steps below go immediately below where the script template says \"======= YOUR CODE HERE ======\".\n",
    "\n",
    "1 - The hypothesis is a vector, formed by multiplying the X matrix and the theta vector. X has size (m x n), and theta is (n x 1), so the product is (m x 1). That's good, because it's the same size as 'y'. Call this hypothesis vector 'h'.\n",
    "\n",
    "2 - The \"errors vector\" is the difference between the 'h' vector and the 'y' vector.\n",
    "\n",
    "3 - The change in theta (the \"gradient\") is the sum of the product of X and the \"errors vector\", scaled by alpha and 1/m. Since X is (m x n), and the error vector is (m x 1), and the result you want is the same size as theta (which is (n x 1), you need to transpose X before you can multiply it by the error vector.\n",
    "\n",
    "The vector multiplication automatically includes calculating the sum of the products.\n",
    "\n",
    "When you're scaling by alpha and 1/m, be sure you use enough sets of parenthesis to get the factors correct.\n",
    "\n",
    "4 - Subtract this \"change in theta\" from the original value of theta. A line of code like this will do it:\n",
    "\n",
    "theta = theta - theta_change;\n",
    "That's it. Since you're never indexing by m or n, this solution works identically for both gradientDescent() and gradientDescentMulti().\n",
    "\n",
    "There is a test case below (or use this link):\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/discussions/-m2ng_KQEeSUBCIAC9QURQ/replies/jCkbzfQsEeSkXCIAC4tJTg\n",
    "\n",
    "===========================\n",
    "\n",
    "Note: Replies to this thread tend to get lost due to a glitch in the forum. Please use the link below to post new questions.\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/discussions/uCXYyH6wEeWU3RJpSD4VQQ\n",
    "\n",
    "The thread you are reading is closed to new comments.\n",
    "\n",
    "===========================\n",
    "\n",
    "Keywords: ex1 tutorial gradientdescent gradientdescentmulti gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load gradientDescent.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)\n",
    "%GRADIENTDESCENT Performs gradient descent to learn theta\n",
    "%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by \n",
    "%   taking num_iters gradient steps with learning rate alpha\n",
    "\n",
    "% Initialize some useful values\n",
    "m = length(y); % number of training examples\n",
    "J_history = zeros(num_iters, 1);\n",
    "\n",
    "for iter = 1:num_iters\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Perform a single gradient step on the parameter vector\n",
    "    %               theta. \n",
    "    %\n",
    "    % Hint: While debugging, it can be useful to print out the values\n",
    "    %       of the cost function (computeCost) and gradient here.\n",
    "    %\n",
    "\n",
    "    h = X * theta; % hypothesis h: is a vector, formed by multiplying the X matrix and the theta vector\n",
    "    errors = h - y; % The \"errors vector\" is the difference between the 'h' vector and the 'y' vector.    \n",
    "    theta_change = alpha / m * (X' * errors); % The change in theta is the sum of the product of X and the \"errors vector\"                        \n",
    "    theta = theta - theta_change;\n",
    "\n",
    "    % ============================================================\n",
    "\n",
    "    % Save the cost J in every iteration    \n",
    "    J_history(iter) = computeCost(X, y, theta);\n",
    "\n",
    "end\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) ex1_multi.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load ex1_multi.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class\n",
    "%  Exercise 1: Linear regression with multiple variables\n",
    "%\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear regression exercise. \n",
    "%\n",
    "%  You will need to complete the following functions in this \n",
    "%  exericse:\n",
    "%\n",
    "%     warmUpExercise.m\n",
    "%     plotData.m\n",
    "%     gradientDescent.m\n",
    "%     computeCost.m\n",
    "%     gradientDescentMulti.m\n",
    "%     computeCostMulti.m\n",
    "%     featureNormalize.m\n",
    "%     normalEqn.m\n",
    "%\n",
    "%  For this part of the exercise, you will need to change some\n",
    "%  parts of the code below for various experiments (e.g., changing\n",
    "%  learning rates).\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "\n",
    "%% ================ Part 1: Feature Normalization ================\n",
    "\n",
    "%% Clear and Close Figures\n",
    "clear ; close all; clc\n",
    "\n",
    "fprintf('Loading data ...\\n');\n",
    "\n",
    "%% Load Data\n",
    "data = load('ex1data2.txt');\n",
    "X = data(:, 1:2);\n",
    "y = data(:, 3);\n",
    "m = length(y);\n",
    "\n",
    "% Print out some data points\n",
    "fprintf('First 10 examples from the dataset: \\n');\n",
    "fprintf(' x = [%.0f %.0f], y = %.0f \\n', [X(1:10,:) y(1:10,:)]');\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "% Scale features and set them to zero mean\n",
    "fprintf('Normalizing Features ...\\n');\n",
    "\n",
    "[X mu sigma] = featureNormalize(X);\n",
    "\n",
    "% Add intercept term to X\n",
    "X = [ones(m, 1) X];\n",
    "\n",
    "\n",
    "%% ================ Part 2: Gradient Descent ================\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: We have provided you with the following starter\n",
    "%               code that runs gradient descent with a particular\n",
    "%               learning rate (alpha). \n",
    "%\n",
    "%               Your task is to first make sure that your functions - \n",
    "%               computeCost and gradientDescent already work with \n",
    "%               this starter code and support multiple variables.\n",
    "%\n",
    "%               After that, try running gradient descent with \n",
    "%               different values of alpha and see which one gives\n",
    "%               you the best result.\n",
    "%\n",
    "%               Finally, you should complete the code at the end\n",
    "%               to predict the price of a 1650 sq-ft, 3 br house.\n",
    "%\n",
    "% Hint: By using the 'hold on' command, you can plot multiple\n",
    "%       graphs on the same figure.\n",
    "%\n",
    "% Hint: At prediction, make sure you do the same feature normalization.\n",
    "%\n",
    "\n",
    "fprintf('Running gradient descent ...\\n');\n",
    "\n",
    "% Choose some alpha value\n",
    "%alpha = 0.01;\n",
    "%num_iters = 400;\n",
    "\n",
    "alpha = 0.1;\n",
    "num_iters = 50;\n",
    "\n",
    "% Init Theta and Run Gradient Descent \n",
    "theta = zeros(3, 1);\n",
    "[theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters);\n",
    "\n",
    "% Plot the convergence graph\n",
    "figure;\n",
    "plot(1:numel(J_history), J_history, '-b', 'LineWidth', 2);\n",
    "xlabel('Number of iterations');\n",
    "ylabel('Cost J');\n",
    "\n",
    "% Display gradient descent's result\n",
    "fprintf('Theta computed from gradient descent: \\n');\n",
    "fprintf(' %f \\n', theta);\n",
    "fprintf('\\n');\n",
    "\n",
    "% Estimate the price of a 1650 sq-ft, 3 br house\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Recall that the first column of X is all-ones. Thus, it does\n",
    "% not need to be normalized.\n",
    "% price = 0; % You should change this\n",
    "price = ([1, 1650, 3] - [0 mu]) ./ [1 sigma] * theta;\n",
    "\n",
    "% ============================================================\n",
    "\n",
    "fprintf(['Predicted price of a 1650 sq-ft, 3 br house ' ...\n",
    "         '(using gradient descent):\\n $%f\\n'], price);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%% ================ Part 3: Normal Equations ================\n",
    "\n",
    "fprintf('Solving with normal equations...\\n');\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: The following code computes the closed form \n",
    "%               solution for linear regression using the normal\n",
    "%               equations. You should complete the code in \n",
    "%               normalEqn.m\n",
    "%\n",
    "%               After doing so, you should complete this code \n",
    "%               to predict the price of a 1650 sq-ft, 3 br house.\n",
    "%\n",
    "\n",
    "%% Load Data\n",
    "data = csvread('ex1data2.txt');\n",
    "X = data(:, 1:2);\n",
    "y = data(:, 3);\n",
    "m = length(y);\n",
    "\n",
    "% Add intercept term to X\n",
    "X = [ones(m, 1) X];\n",
    "\n",
    "% Calculate the parameters from the normal equation\n",
    "theta = normalEqn(X, y);\n",
    "\n",
    "% Display normal equation's result\n",
    "fprintf('Theta computed from the normal equations: \\n');\n",
    "fprintf(' %f \\n', theta);\n",
    "fprintf('\\n');\n",
    "\n",
    "% Estimate the price of a 1650 sq-ft, 3 br house\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% price = 0; % You should change this\n",
    "price = [1 1650 3] * theta;\n",
    "\n",
    "\n",
    "% ============================================================\n",
    "\n",
    "fprintf(['Predicted price of a 1650 sq-ft, 3 br house ' ...\n",
    "         '(using normal equations):\\n $%f\\n'], price);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Solution of ex1_multi.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) featureNormalize.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/machine-learning/module/vW94N/discussions/VBS-KfMxEeSUBCIAC9QURQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Normalization w/o loop**\n",
    "\n",
    "There are a couple of methods to accomplish this. The method here is one I use that doesn't rely on automatic broadcasting or the bsxfun() function.\n",
    "\n",
    "You can use the mean() and sigma() functions to get the mean and std deviation for each column of X. These are returned as row vectors (1 x n)\n",
    "Now you want to apply those values to each element in every row of the X matrix. One way to do this is to duplicate these vectors for each row in X, so they're the same size.\n",
    "One method to do this is to create a column vector of all-ones - size (m x 1) - and multiply it by the mu or sigma row vector (1 x n). Dimensionally, (m x 1) * (1 x n) gives you a (m x n) matrix, and every row of the resulting matrix will be identical.\n",
    "\n",
    "Now that X, mu, and sigma are all the same size, you can use element-wise operators to compute X_normalized.\n",
    "Try these commands in your workspace:\n",
    "\n",
    "```octave\n",
    "X = [1 2 3; 4 5 6]        % creates a test matrix\n",
    "mu = mean(X)              % returns a row vector\n",
    "sigma = std(X)            % returns a row vector\n",
    "m = size(X, 1)            % returns the number of rows in X\n",
    "mu_matrix = ones(m, 1) * mu  \n",
    "sigma_matrix = ones(m, 1) * sigma\n",
    "```\n",
    "Now you can subtract the mu matrix from X, and divide element-wise by the sigma matrix, and arrive at X_normalized.\n",
    "\n",
    "You can do this even easier if you're using a Matlab or Octave version that supports automatic broadcasting - then you can skip the \"multiply by a column of 1's\" part. I haven't got one of those available at the moment to test the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load featureNormalize.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function [X_norm, mu, sigma] = featureNormalize(X)\n",
    "%FEATURENORMALIZE Normalizes the features in X \n",
    "%   FEATURENORMALIZE(X) returns a normalized version of X where\n",
    "%   the mean value of each feature is 0 and the standard deviation\n",
    "%   is 1. This is often a good preprocessing step to do when\n",
    "%   working with learning algorithms.\n",
    "\n",
    "% You need to set these values correctly\n",
    "X_norm = X;\n",
    "mu = zeros(1, size(X, 2));\n",
    "sigma = zeros(1, size(X, 2));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: First, for each feature dimension, compute the mean\n",
    "%               of the feature and subtract it from the dataset,\n",
    "%               storing the mean value in mu. Next, compute the \n",
    "%               standard deviation of each feature and divide\n",
    "%               each feature by it's standard deviation, storing\n",
    "%               the standard deviation in sigma. \n",
    "%\n",
    "%               Note that X is a matrix where each column is a \n",
    "%               feature and each row is an example. You need \n",
    "%               to perform the normalization separately for \n",
    "%               each feature. \n",
    "%\n",
    "% Hint: You might find the 'mean' and 'std' functions useful.\n",
    "%       \n",
    "\n",
    "% Method 1: Using for-loop\n",
    "%{\n",
    "mu = mean(X); % mean of the feature\n",
    "sigma = std(X); % standard deviation\n",
    "for i = 1:size(X,2)\n",
    "    X_norm(:,i) = (X(:,i) - mu(i)) / sigma(i);\n",
    "end\n",
    "%}\n",
    "\n",
    "\n",
    "% Method 2: Without for-loop\n",
    "mu = mean(X);              % returns a row vector\n",
    "sigma = std(X);            % returns a row vector\n",
    "m = size(X, 1);            % returns the number of rows in X\n",
    "mu_matrix = ones(m, 1) * mu; \n",
    "sigma_matrix = ones(m, 1) * sigma;\n",
    "\n",
    "% subtract the mu matrix from X, and divide element-wise by the sigma matrix, \n",
    "% and arrive at X_normalized\n",
    "X_norm = (X - mu_matrix) ./ sigma_matrix; \n",
    "\n",
    "% ============================================================\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) computeCostMulti.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load computeCostMulti.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function J = computeCostMulti(X, y, theta)\n",
    "%COMPUTECOSTMULTI Compute cost for linear regression with multiple variables\n",
    "%   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the\n",
    "%   parameter for linear regression to fit the data points in X and y\n",
    "\n",
    "% Initialize some useful values\n",
    "m = length(y); % number of training examples\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "J = 0;\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Compute the cost of a particular choice of theta\n",
    "%               You should set J to the cost.\n",
    "\n",
    "\n",
    "J = (X*theta - y)' * (X*theta - y) / (2*m);\n",
    "\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) gradientDescentMulti.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load gradientDescentMulti.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "%GRADIENTDESCENTMULTI Performs gradient descent to learn theta\n",
    "%   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by\n",
    "%   taking num_iters gradient steps with learning rate alpha\n",
    "\n",
    "% Initialize some useful values\n",
    "m = length(y); % number of training examples\n",
    "J_history = zeros(num_iters, 1);\n",
    "\n",
    "for iter = 1:num_iters\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Perform a single gradient step on the parameter vector\n",
    "    %               theta. \n",
    "    %\n",
    "    % Hint: While debugging, it can be useful to print out the values\n",
    "    %       of the cost function (computeCostMulti) and gradient here.\n",
    "    %\n",
    "\n",
    "    h = X * theta; % hypothesis h: is a vector, formed by multiplying the X matrix and the theta vector\n",
    "    errors = h - y; % The \"errors vector\" is the difference between the 'h' vector and the 'y' vector.    \n",
    "    theta_change = alpha / m * (X' * errors); % The change in theta is the sum of the product of X and the \"errors vector\"                        \n",
    "    theta = theta - theta_change;\n",
    "\n",
    "\n",
    "    % ============================================================\n",
    "\n",
    "    % Save the cost J in every iteration    \n",
    "    J_history(iter) = computeCostMulti(X, y, theta);\n",
    "\n",
    "end\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) normalEqn.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load normalEqn.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function [theta] = normalEqn(X, y)\n",
    "%NORMALEQN Computes the closed-form solution to linear regression \n",
    "%   NORMALEQN(X,y) computes the closed-form solution to linear \n",
    "%   regression using the normal equations.\n",
    "\n",
    "theta = zeros(size(X, 2), 1);\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Complete the code to compute the closed form solution\n",
    "%               to linear regression and put the result in theta.\n",
    "%\n",
    "\n",
    "% ---------------------- Sample Solution ----------------------\n",
    "\n",
    "theta = pinv(X' * X) * X' * y;\n",
    "\n",
    "\n",
    "% -------------------------------------------------------------\n",
    "\n",
    "\n",
    "% ============================================================\n",
    "\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave_kernel"
  },
  "language_info": {
   "codemirror_mode": "Octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
