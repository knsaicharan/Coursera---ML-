{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1) Multiple Features](#1%29-Multiple-Features)\n",
    "\t* [1) Notation](#1%29-Notation)\n",
    "\t* [2) Hypothesis](#2%29-Hypothesis)\n",
    "* [2) Gradient Descent for Multiple Variables](#2%29-Gradient-Descent-for-Multiple-Variables)\n",
    "\t* [New algorithm](#New-algorithm)\n",
    "* [3) Gradient Descent in Practice: Feature Scaling](#3%29-Gradient-Descent-in-Practice:-Feature-Scaling)\n",
    "\t* [1) Idea: Make sure features are on a similar scale.](#1%29-Idea:-Make-sure-features-are-on-a-similar-scale.)\n",
    "\t* [2) Mean Normalization](#2%29-Mean-Normalization)\n",
    "* [4) Gradient Descent in Practice: Learning Rate](#4%29-Gradient-Descent-in-Practice:-Learning-Rate)\n",
    "\t* [1) Making sure gradient descent is working correctly](#1%29-Making-sure-gradient-descent-is-working-correctly)\n",
    "\t* [2) Summary](#2%29-Summary)\n",
    "* [5) Features and polynomial regression](#5%29-Features-and-polynomial-regression)\n",
    "\t* [1) Polynomial regression](#1%29-Polynomial-regression)\n",
    "\t* [2) Choice of features](#2%29-Choice-of-features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Multiple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic1.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/6Nj1q/multiple-features) 3:05*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic2.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/6Nj1q/multiple-features) 3:24*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For convenience of notation, we define $\\large x_{0} = 1$.\n",
    "- Previously, we had n features starting from $\\large x_{1}, x_{2} ... x_{n}$. Now after we define an additional zero feature, now feature vector x becomes n + 1 dimensional vector that is zero index.\n",
    "- For the parameter vector $\\large \\theta$, again, this is another zero index vector staring from $\\large \\theta_{0}$, so this is another n + 1 dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic3.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/6Nj1q/multiple-features) 8:15*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\large \\theta^{T}$ is a (1 by n) matrix\n",
    "- x is a (n by 1) matrix\n",
    "- The result of $\\large \\theta^{T}x$ is a (1 by 1) constant value\n",
    "\n",
    "The form of the hypothesis is just the inner product between our parameter vector theta and our theta vector X.\n",
    "\n",
    "The term multivariable is just maybe a fancy term for saying we have multiple features, or multivariables with which to try to predict the value Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Gradient Descent for Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic4.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/Z9DKX/gradient-descent-for-multiple-variables) 1:20*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this quiz, we have:\n",
    "$$\\large h_{\\theta}(x) = \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n} = \\theta^{T}x$$\n",
    "\n",
    "    - Answer 1:  $\\large h_{\\theta}(x^{(i)}) = \\theta^{T}x^{(i)} $\n",
    "    - Answer 2:\n",
    "    $\\large h_{\\theta}(x^{(i)})= \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n} = (\\sum_{j=0}^n \\theta_{j}x_{j}^{(i)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic5.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/Z9DKX/gradient-descent-for-multiple-variables) 1:47*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new and old algorithms are both similar to each other. If you consider a case where we have two features, then we have three update rules for the parameters $\\theta_{0}, \\theta_{1}, \\theta_{2}$.\n",
    "\n",
    "- If you look at the update rule for $\\theta_{0}$, what you find is that this update rule here is the same as the update rule that we had previously for the case of n = 1. Because in our notational convention we had $x_{0}^{(i)} = 1$.\n",
    "- Same thing if you compare the new $\\theta_{1}$ equation with the old one, you find them similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic6.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/Z9DKX/gradient-descent-for-multiple-variables) 4:57*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Gradient Descent in Practice: Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Idea: Make sure features are on a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if you have 2 feafures:\n",
    "- x1: size of the house\n",
    "- x2: number of bedrooms\n",
    "\n",
    "If those feature are not on a similar scale, when you plot the contours of the cost function $\\large J_{\\theta}$, you will have a skewed elliptical shape. And if you run gradient descents on this cost function, your gradients may end up taking a long time and can oscillate back and forth, before it can finally find its way to the global minimum.\n",
    "\n",
    "When you divide x1 by 2000, and x2 by 5, you have a less skewed contour cost function. Also when you run the gradient descent, you can find a much more direct path to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/lec4_pic7.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling) 3:00*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure features are on a similar scale.**\n",
    "\n",
    "- Get every feature into approximately a $-1 <= x_{i} <= 1$ range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_{i}$: feature\n",
    "- $\\mu_{i}$: mean (avarage) value of x (feature)\n",
    "- $s_{i}$: range of the feature (max - min) or you can set $s_{i}$ as the standard deviation of the feature.\n",
    "\n",
    "$$\\large x_{i} = \\dfrac{x_{i} - \\mu_{i}}{s_{i}}$$\n",
    "\n",
    "For example,\n",
    "- $x_{1}$: size of the house. We have the average size of a house is 1000. Then we set feature $x_{1} = \\dfrac{size - 1000}{2000}$\n",
    "- $x_{2}$: number of bedrooms. We have the average size of a house is 2. Then $x_{2} = \\dfrac{bedrooms - 2}{5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic8.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling) 8:15*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic9.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling) 8:35*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Gradient Descent in Practice: Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Making sure gradient descent is working correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If gradient descent is working properly, then the cost function $J_{\\theta}$ should decrease after every iteration.\n",
    "\n",
    "One useful thing in the plot below is that it looks like by the time you've gotten out to maybe 300 iterations, between 300 and 400 iterations, $J_{\\theta}$ hasn't gone down much more. By the time you get to 400 iterations, it looks like this curve has flattened out here. This means that at 400 iterations, gradient descent has more or less <u>**converged**</u> because your cost function isn't going down much more.\n",
    "\n",
    "So, looking at this figure can also help you judge whether or not gradient descent has converged. Also, the number of iterations\n",
    "the gradient descent takes to converge for a physical application can vary a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic10.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate) 4:22*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see $J_{\\theta}$ is actually increasing, then that gives you a clear sign that gradient descent is not working. It usually means that you should decrease the learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic11.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate) 6:50*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic12.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate) 6:51*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I can do when I try to run gradient descent is I would try a range of values, for example: 0.001, 0.003, 0.01, ... until I find a good learning rate for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic13.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate) 8:35*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Features and polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Polynomial regression allows you to use the machinery of linear regression to fit very complicated, even very non-linear functions.\n",
    "\n",
    "Let's take the example of predicting the price of the house. Suppose the house has 2 features: frontage and depth. You might build a linear regression model using frontage as your first feature x1 and depth is your second feature x2. But when you apply linear regression, you can actually create a new feature: area of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic14.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/Rqgfz/features-and-polynomial-regression) 2:03*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an example below:\n",
    "\n",
    "- It doesn't look like a straight line fits this data very well. So maybe you want to fit a **<u>quadratic model</u>** which will give you a better fit. But then you may decide that your quadratic model doesn't make sense because of a quadratic function, eventually this function comes back down and we don't think housing prices should go down when the size goes up too high.\n",
    "\n",
    "- Then, we may choose to use a **<u>cubic function</u>**, and where we have now a third-order term and we fit that, the green line is a better fit to the data cause it doesn't eventually come back down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic15.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/Rqgfz/features-and-polynomial-regression) 2:03*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we want to fit a model with the cubic function, we need to have 3 features:\n",
    "- $x_{1}:$ size of the house\n",
    "- $x_{2}:$ square of size of the house\n",
    "- $x_{1}:$ the cube of size of the house\n",
    "\n",
    "And, just by choosing my three features this way and applying the machinery of linear regression, I can fit this model and end up with a cubic fit to my data.\n",
    "\n",
    "**<u>Feature scaling</u>** becomes increasingly important when we choose features like this. Because the three features take on a very different ranges of values, it's important to apply feature scaling if you're using gradient descent to get them into comparable ranges of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Choice of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than going to use a **<u>cubic model</u>**, you can choose to use a **<u>square root function</u>**, then you end up with the curve flattens out a bit and doesn't ever come back down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic16.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/Rqgfz/features-and-polynomial-regression) 6:32*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic17.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/Rqgfz/features-and-polynomial-regression) 6:35*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
